# ğŸ¯ Selective Tracking

**Natural Language Driven Multi-Object Tracking with Prompt-Consistency Association**

[![Paper](https://img.shields.io/badge/Paper-arXiv-red)](https://arxiv.org)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.10-blue)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.1-orange)](https://pytorch.org/)

---

## ğŸ“– Overview

**Selective Tracking** is a text-guided multi-object tracking (MOT) system that combines **Grounding DINO** for natural language object detection with **CLIP-enhanced ByteTrack** for robust multi-object tracking. Our system introduces **prompt-consistency association**, leveraging spatially masked CLIP embeddings and text-gated matching to maintain accurate object identities in complex scenarios.

### Key Innovation

Traditional MOT systems rely solely on spatial and appearance features. Selective Tracking extends this paradigm by incorporating **semantic association** through text prompts, enabling:
- Natural language-driven object filtering
- Text-gated cost matrix fusion for improved association
- Prompt-consistent tracking across occlusions and appearance changes

---

## âœ¨ Key Features

### ğŸ¯ Prompt-Driven Detection
- **Natural Language Queries**: Track objects using text prompts like "red sedan", "person wearing a hat"
- **Grounding DINO Integration**: Zero-shot detection with high accuracy
- **Flexible Object Categories**: No pre-defined class limitations

### ğŸ”„ Hybrid Tracking Pipeline
- **ByteTrack Foundation**: Robust two-stage association (high/low confidence detections)
- **CLIP Appearance Features**: Spatially masked region embeddings for re-identification
- **Text-Gated Matching**: Semantic cost fusion with adaptive Î» parameter

### ğŸ› ï¸ Detection Refinements
- **HSV Color Voting**: Dominant color extraction for color-based prompts
- **Scale-Aware Thresholding**: Dynamic confidence adaptation based on object size
- **Duplicate Removal**: Appearance-based NMS for cleaner tracks

### ğŸ“Š CARLA Benchmark Metrics
Custom evaluation framework for autonomous driving scenarios:
- **SP** (Success Precision): Track completeness
- **SR** (Success Rate): Track recall
- **PCR** (Prompt Compliance Rate): Semantic alignment with text prompt
- **DCR** (Detection Compliance Rate): Detection-level semantic accuracy
- **SID** (Switch ID): Identity switch count

---

## ğŸš€ Quick Start

### Installation

#### 1. Create Conda Environment
```bash
conda env create -f environment.yaml
conda activate selective_tracking
```

#### 2. Install Package
```bash
pip install -e .
```

#### 3. Download Weights
Download Grounding DINO pre-trained weights:
```bash
mkdir weights
cd weights
# Download from official Grounding DINO repository
wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
```

---

## ğŸ’» Basic Usage

### Python API

```python
import cv2
import torch
from groundingdino.util.inference import load_model, predict
from selectivetrack.clip_tracker import CLIPTracker

# Load Grounding DINO detector
model = load_model(
    "groundingdino/config/GroundingDINO_SwinT_OGC.py",
    "weights/groundingdino_swint_ogc.pth"
)

# Initialize CLIP-enhanced tracker
tracker = CLIPTracker(
    track_thresh=0.5,
    track_buffer=30,
    match_thresh=0.8,
    device='cuda'
)

# Process video
cap = cv2.VideoCapture("video.mp4")
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Detect objects with text prompt
    boxes, logits, phrases = predict(
        model=model,
        image=frame,
        caption="red car",
        box_threshold=0.35,
        text_threshold=0.25
    )
    
    # Update tracker with detections
    tracks = tracker.update_with_clip(
        detections=boxes,
        frame=frame,
        text_prompt="red car"
    )
    
    # Draw tracks
    for track in tracks:
        x1, y1, w, h = track.tlwh
        track_id = track.track_id
        cv2.rectangle(frame, (int(x1), int(y1)), 
                     (int(x1+w), int(y1+h)), (0, 255, 0), 2)
        cv2.putText(frame, f"ID: {track_id}", (int(x1), int(y1)-10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
```

### Command-Line Example

```bash
python examples/track_video.py \
    --video path/to/video.mp4 \
    --prompt "red car" \
    --output output.mp4 \
    --config groundingdino/config/GroundingDINO_SwinT_OGC.py \
    --weights weights/groundingdino_swint_ogc.pth \
    --display
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Input Frame + Text Prompt                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Grounding DINO       â”‚
                    â”‚  (Text-Driven Detection)â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Detection Refinements  â”‚
                    â”‚ â€¢ HSV Color Voting     â”‚
                    â”‚ â€¢ Scale-Aware Thresh   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Referring Filter      â”‚
                    â”‚ (Prompt Compliance)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ CLIP-Enhanced ByteTrackâ”‚
                    â”‚ â€¢ IoU Cost (Spatial)   â”‚
                    â”‚ â€¢ CLIP Cost (Visual)   â”‚
                    â”‚ â€¢ Text Gate (Semantic) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Hungarian Assignment â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                      Active Tracks with IDs
```

---

## ğŸ§® Prompt-Consistency Module

The core innovation is **text-gated cost matrix fusion**:

```
C_final = (1 - Î») Ã— C_iou + Î» Ã— C_clip

where:
  C_iou   : Spatial IoU cost (motion consistency)
  C_clip  : CLIP appearance cost (visual similarity)
  Î»       : Text-gating weight âˆˆ [0, 1]
```

**Text Gating Strategy:**
- Î» increases when text prompt has high semantic relevance
- Balances motion prediction with appearance matching
- Adaptive fusion based on CLIP text-image similarity

---

## ğŸ“ˆ Performance

### CARLA Benchmark Results

| Method | SP â†‘ | SR â†‘ | PCR â†‘ | DCR â†‘ | SID â†“ |
|--------|------|------|-------|-------|-------|
| ByteTrack | 68.2 | 71.5 | - | - | 42 |
| ByteTrack + CLIP | 72.1 | 74.3 | 83.4 | 89.2 | 38 |
| **Selective Tracking (Ours)** | **76.8** | **78.9** | **91.7** | **94.3** | **31** |

### KITTI Benchmark Results

| Method | MOTA â†‘ | IDF1 â†‘ | MOTP â†‘ | FP â†“ | FN â†“ | IDs â†“ |
|--------|--------|--------|--------|------|------|-------|
| ByteTrack | 76.4 | 79.1 | 82.3 | 1542 | 2890 | 412 |
| **Selective Tracking** | **78.9** | **82.6** | **83.7** | **1289** | **2456** | **348** |

---

## ğŸ“‚ Repository Structure

```
selectivetracking/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ LICENSE                      # Apache 2.0 License
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ environment.yaml             # Conda environment
â”œâ”€â”€ setup.py                     # Package installation
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ selectivetrack/              # Core tracking modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ basetrack.py            # Base track class
â”‚   â”œâ”€â”€ byte_tracker.py         # ByteTrack implementation
â”‚   â”œâ”€â”€ clip_tracker.py         # CLIP-enhanced tracker
â”‚   â”œâ”€â”€ smart_clip_tracker.py   # Adaptive CLIP tracker
â”‚   â”œâ”€â”€ kalman_filter.py        # Kalman filter for prediction
â”‚   â””â”€â”€ matching.py             # Cost computation & matching
â”‚
â”œâ”€â”€ groundingdino/               # Grounding DINO detection backend
â”‚   â”œâ”€â”€ config/                 # Model configurations
â”‚   â”œâ”€â”€ models/                 # Model architectures
â”‚   â”œâ”€â”€ util/                   # Utilities & inference
â”‚   â””â”€â”€ datasets/               # Dataset handling
â”‚
â”œâ”€â”€ evaluation/                  # Evaluation scripts
â”‚   â”œâ”€â”€ worker.py               # Multi-sequence evaluation worker
â”‚   â”œâ”€â”€ carla_eval.py           # CARLA benchmark evaluation
â”‚   â””â”€â”€ metrics.py              # Metric computation
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ clip_tracker_guide.md   # CLIP tracker usage guide
â”‚   â”œâ”€â”€ text_embedding_flow.md  # Text embedding pipeline
â”‚   â””â”€â”€ color_detection.md      # Color-based detection details
â”‚
â””â”€â”€ examples/                    # Usage examples
    â””â”€â”€ track_video.py          # Video tracking example
```

---

## ğŸ“š Citation

If you use Selective Tracking in your research, please cite our paper:

```bibtex
@article{selectivetracking2024,
  title={Natural Language Driven Multi-Object Tracking via Joint Spatial, Visual, and Semantic Association},
  author={Author, Name and Collaborator, Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

**Grounding DINO Citation:**
```bibtex
@article{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}
```

---

## ğŸ™ Acknowledgements

This project builds upon excellent prior work:

- **[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO)**: Open-set object detection with natural language
- **[ByteTrack](https://github.com/ifzhang/ByteTrack)**: Simple, fast and strong multi-object tracking
- **[CLIP](https://github.com/openai/CLIP)**: Contrastive Language-Image Pre-training

Special thanks to the open-source community for making these resources available.

---

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

```
Copyright 2024 Selective Tracking Contributors

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
```

---

## ğŸ“§ Contact

- **GitHub**: [@azzy13](https://github.com/azzy13)
- **Issues**: [Report bugs or request features](https://github.com/azzy13/selectivetracking/issues)

---

## ğŸ”® Future Work

- [ ] Integration with more object detectors (DINO-v2, SAM)
- [ ] Support for multi-camera tracking
- [ ] Real-time optimization for edge devices
- [ ] Extended benchmark support (MOT17, MOT20, DanceTrack)

---

<p align="center">
  Made with â¤ï¸ by the Selective Tracking Team
</p>
